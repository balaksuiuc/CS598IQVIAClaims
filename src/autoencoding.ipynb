#%% md

# Autoencoder

1. Build the Autoencoder model
2. Get the encoding out
3. Use encoding to train model for prediction
4. Check if dimensionality reduction has reduced overfitting without effecting model performance


#%% md

---

#%%

import os
import pickle
import random
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import torch.nn.functional as F
import pandas as pd

# set seed
seed = 100

def set_seed(seed):
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)

set_seed(seed)

EMB_DATA_PATH = "/Users/kkhare/Downloads/"

assert os.path.isdir(EMB_DATA_PATH)

#%% md

---

#%%

!ls $EMB_DATA_PATH/x_18474_3336.csv
!ls $EMB_DATA_PATH/ycharge.csv


#%% md

### Load data

#%%

import time
import pandas as pd
import numpy as np
import datetime

# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.

def read_emb_csv(filepath):
    #pd_x = pd.read_csv(filepath + 'x_18474_3348.csv')
    pd_x = pd.read_csv(filepath + 'x_18474_3336.csv')
    pd_y_charge = pd.read_csv(filepath + 'ycharge.csv')
    pd_y_paid = pd.read_csv(filepath + 'ypaid.csv')

    return pd_x, pd_y_charge, pd_y_paid

# load the emb csv
pd_x, pd_y_charge, pd_y_paid = read_emb_csv(EMB_DATA_PATH)

#%%

print("pd_x: {}".format(pd_x.shape))
print("pd_y_charge: {}".format(pd_y_charge.shape))
print("pd_y_paid: {}".format(pd_y_paid.shape))

#%%

# Input data normalization
# https://betashort-lab.com/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%AD%A3%E8%A6%8F%E5%8C%96%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%89%8D%E5%87%A6%E7%90%86/#toc4

from sklearn.preprocessing import MinMaxScaler

if 1:
    scaler_mm = MinMaxScaler()
    scaler_mm.fit(pd_x)
    pd_x = pd.DataFrame(scaler_mm.transform(pd_x))

#%%

# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, x, y):
        # convert to torch.tensor
        self.x = torch.tensor(x.values.astype(np.float32))
        self.y = torch.tensor(y.values.astype(np.float32))

    def __len__(self):
        return len(self.y)

    def __getitem__(self, index):
        #return self.x.loc[index][:], self.y.loc[index]
        return self.x[index][:], self.y[index]


#%%

dataset = CustomDataset(pd_x, pd_y_paid)
print(len(dataset))
print(dataset[0][0])
print(dataset[0][0].shape)
#print(dataset[0][1].shape)

print(dataset.x.shape)
print(dataset.y.shape)

#%% md


### Define Autoencoder model

#%%

# https://jamesmccaffrey.wordpress.com/2021/04/01/visualizing-the-mnist-dataset-using-pytorch-autoencoder-dimensionality-reduction/
class Autoencoder(nn.Module):
  def __init__(self):
    super(Autoencoder, self).__init__()
    self.layer1 = nn.Linear(3336, 256)  
    self.layer2 = nn.Linear(256, 128)
    self.layer3 = nn.Linear(128, 256)
    self.layer4 = nn.Linear(256, 3336)

  def encode(self, x):
    z = torch.tanh(self.layer1(x))
    z = torch.tanh(self.layer2(z))   
    return z

  def decode(self, x):
    z = torch.tanh(self.layer3(x))
    z = torch.tanh(self.layer4(z))  
    return z

  def forward(self, x):
    z = self.encode(x)
    oupt = self.decode(z)
    return oupt

#%% md

### Autoencoder Training

#%%

# https://jamesmccaffrey.wordpress.com/2021/04/01/visualizing-the-mnist-dataset-using-pytorch-autoencoder-dimensionality-reduction/
def train(ae, ds, bs, me, lr, le):
  # train autoencoder ae with dataset ds using batch size bs,
  # with max epochs me, learn rate lr, log_every le
  data_ldr = torch.utils.data.DataLoader(ds, batch_size=bs,
    shuffle=True)

  loss_func = torch.nn.MSELoss()
  opt = torch.optim.Adam(ae.parameters(), lr=lr)
  print("Starting training")
  for epoch in range(0, me):
    print("epoch:", epoch)
    for (b_idx, batch) in enumerate(data_ldr):
      opt.zero_grad()
      X = batch[0]
      #print(X.shape)
      oupt = ae(X)
      loss_val = loss_func(oupt, X)
      loss_val.backward()
      opt.step()

    if epoch != 0 and epoch % le == 0:
      print("epoch = %6d" % epoch, end="")
      print("  curr batch loss = %7.4f" % \
loss_val.item(), end="")
      print("")
  print("Training complete ")

#%% md

### Do training

#%%

# https://jamesmccaffrey.wordpress.com/2021/04/01/visualizing-the-mnist-dataset-using-pytorch-autoencoder-dimensionality-reduction/
def main():

  # create and train autoencoder model
  print("\nCreating autoencoder using  \n")
  autoenc = Autoencoder()
  autoenc.train()

  bat_size = 32
  max_epochs = 30
  lrn_rate = 0.001
  log_every = 1
  train(autoenc, dataset, bat_size, max_epochs, \
    lrn_rate, log_every)

  # use model encoder to generate reduced dimension data
  autoenc.eval()
  inputData = dataset.x

  with torch.no_grad():
    dataset.x = autoenc.encode(inputData)

  print("\nfeatures with reduced dimensions")
  print(dataset.x.shape)


#%%

if __name__ == "__main__":
  main()

#%% md

### Reduced dimension dataset

#%%

print(dataset.x.shape)

#%% md

### Splitting the data in Training, Validation and Test

#%%

# https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision
# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab

from torch.utils.data.dataset import random_split

train_len = int(len(dataset)*0.7)
val_len = int(len(dataset)*0.2)
lengths = [train_len, val_len, len(dataset) - train_len - val_len]
train_dataset, val_dataset, test_dataset = random_split(dataset, lengths)

print("train data length: {}".format(len(train_dataset)))
print("val data length: {}".format(len(val_dataset)))
print("test data length: {}".format(len(test_dataset)))

#%% md

### Loading data for training

#%%

# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab
from torch.utils.data import DataLoader

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Test
# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab

train_iter = iter(train_loader)
x, y = next(train_iter)

print('Shape of a batch x:', x.shape)
print('Shape of a batch y:', y.shape)

#%% md

### Model building on reduced dimension dataset


#%%

# Defining the model
class Adv_model(nn.Module):
    def __init__(self, input_size=128, output_size=1):
        super(Adv_model, self).__init__()
        self.input_size = input_size
        self.hidden_size = 50
        self.output_size = output_size

        self.fc1 = torch.nn.Linear(in_features=self.input_size, out_features=self.hidden_size)
        self.fc2 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)
        self.fc3 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)
        #self.fc4 = torch.nn.Linear(in_features=self.hidden_size+self.input_size, out_features=self.output_size)
        self.fc4 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.output_size)
        self.do = nn.Dropout(0.25)

    def forward(self, x):
        x0 = x
        x = self.do(F.relu(self.fc1(x)))
        x = self.do(F.relu(self.fc2(x)))
        x = self.do(F.relu(self.fc3(x)))
        #x = torch.cat((x, x0), dim=1)
        x = self.do(F.relu(self.fc4(x)))

        return x

#%% md

### Model Selection

#%%

# Select which model to use

#model = Db_model()
#model = New_model()
model = Adv_model()
print(model)

#%% md

### Model Training

#%%

# Loss and Optimizer
import torch.nn as nn

# https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html

criterion = nn.MSELoss()
#criterion = nn.L1Loss()

# Learning rate is not written in the prior papar. => Experimental value
optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)

#%%

# Model Evaluation
# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab
# https://scikit-learn.org/stable/modules/model_evaluation.html

from sklearn.metrics import *

def regression_metrics(Y_pred, Y_True):
    # Evaluation of methods:
    # 1. Pearson's correlation (r),
    # 2. Spearman's correlation (),
    # 3. Mean absolute prediction error (MAPE),
    # 4. R squared (r2),
    # 5. Cumming's Prediction Measure (CPM)
    mae, r2 = mean_absolute_error(Y_True, Y_pred), \
                r2_score(Y_True, Y_pred)

    return mae, r2

def evaluate(model, val_loader):
    model.eval()
    all_y_true = torch.LongTensor()
    all_y_pred = torch.LongTensor()

    val_loss = 0
    for x, y in val_loader:
        y_pred = model(x)

        # convert shape from [batch size, 1] to [batch size]
        y_pred = y_pred.view(y_pred.shape[0])
        y = y.view(y.shape[0])

        all_y_true = torch.cat((all_y_true, y.to('cpu').long()), dim=0)
        all_y_pred = torch.cat((all_y_pred, y_pred.to('cpu').long()), dim=0)

        loss = criterion(y_pred, y)
        val_loss += loss.item()
    val_loss = val_loss / len(val_loader)
    mae, r2 = regression_metrics(all_y_pred, all_y_true)
    #print(f"mape: {mape:.3f}, r2: {r2:.3f}")
    return val_loss, mae, r2

#%%

# test without training
evaluate(model, train_loader)

#%%

# Training
# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab

def train(model, train_loader, val_loader, n_epochs):
    model.train()

    for epoch in range(n_epochs):
        train_loss= 0
        all_y_true = torch.LongTensor()
        all_y_pred = torch.LongTensor()

        for x, y in train_loader:
            optimizer.zero_grad()
            y_pred = model(x)

            #print(y_pred.shape)

            # convert shape from [batch size, 1] to [batch size]
            y_pred = y_pred.view(y_pred.shape[0])
            y_pred[y_pred < 0] = 0

            y = y.view(y.shape[0])
            #print(y_pred.shape)
            #print(y.shape)
            all_y_true = torch.cat((all_y_true, y.to('cpu').long()), dim=0)
            all_y_pred = torch.cat((all_y_pred, y_pred.to('cpu').long()), dim=0)

            loss = criterion(y_pred, y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        train_loss = train_loss / len(train_loader)
        #train_MAE = mean_absolute_error(all_y_true, all_y_pred)
        _, train_MAE, _ = evaluate(model, train_loader)
        val_loss, val_MAE, r2 = evaluate(model, val_loader)
        print('Epoch: {} \t Training Loss: {:.6f} \t Training MAE: {:.6f} \t Validation Loss: {:.6f} \t Validation MAE: {:.6f}'.format(epoch+1, train_loss, train_MAE, val_loss, val_MAE))


#%%

n_epochs = 300 # the prior paper's n_epochs=25
train(model, train_loader, val_loader, n_epochs)

#%% md

### Test

#%%

# Test
_, test_mae, _ = evaluate(model, test_loader)
print('Test MAE: %.2f'%(test_mae))
