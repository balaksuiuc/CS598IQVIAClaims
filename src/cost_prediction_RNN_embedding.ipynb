{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cost_prediction_RNN_embedding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxmuXO6v7cG8qxLDzvM4b7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balaksuiuc/CS598IQVIAClaims/blob/main/src/cost_prediction_RNN_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUW9XPjwQWOu"
      },
      "source": [
        "The following code uses RNN+embedding to predict patient treatment costs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpDRSe7sQccF",
        "outputId": "bb814033-33c0-4250-9224-5d283a965d52"
      },
      "source": [
        "import pandas, numpy\n",
        "import urllib.request\n",
        "import os, datetime\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "import sklearn\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy  as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import sys\n",
        "print(sys.version_info)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skRXW4nzROVe"
      },
      "source": [
        "1.1 Load preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L69de4uDRN3E"
      },
      "source": [
        "# set seed\n",
        "seed = 24\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"]=str(seed)\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/iqvia_data/bala_seq_data/\"\n",
        "DATA_PATH = DATA_DIR\n",
        "\n",
        "#pids  = pandas.read_pickle(os.path.join(DATA_PATH,'pids.pkl'))\n",
        "\n",
        "pids  = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'),'rb'))\n",
        "morts = pickle.load(open(os.path.join(DATA_PATH,'morts.pkl'),'rb')) # this is a list of floats\n",
        "seqs  = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'),'rb'))\n",
        "num_types = len(numpy.unique(list(itertools.chain(*list(itertools.chain(*seqs))))))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzDyy_QMRNT0"
      },
      "source": [
        "1.2 Define custom dataset and collate_fn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErOeyJGeRjD9",
        "outputId": "0d7ef8e3-1fa2-4c5f-bb62-e9fd93d9b5de"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, seqs, morts):\n",
        "        self.x = seqs\n",
        "        self.y = morts\n",
        "    def __len__(self):\n",
        "        return(len(self.x))\n",
        "    def __getitem__(self, index):\n",
        "        return (self.x[index], self.y[index])\n",
        "    \n",
        "## collate_fn\n",
        "def collate_fn(data):\n",
        "    sequences, labels = zip(*data)\n",
        "    y = torch.tensor(labels, dtype=torch.float)\n",
        "    \n",
        "    num_patients = len(sequences)\n",
        "    num_visits = [len(patient) for patient in sequences]\n",
        "    max_num_visits = max(num_visits)\n",
        "    #num_diagcodes = [max([len(np.unique(s)) for s in ss]) for ss in sequences]\n",
        "    num_diagcodes = [max([len(s) for s in ss]) for ss in sequences]\n",
        "    max_num_diagcodes = max(num_diagcodes)\n",
        "    #print('num_patients', num_patients, 'num_visits', num_visits, \\\n",
        "    #      'max_num_visits', max_num_visits, 'num_diagcodes', num_diagcodes, \\\n",
        "    #          'max_num_diagcodes', max_num_diagcodes)\n",
        "    \n",
        "    x = torch.zeros((num_patients, max_num_visits, max_num_diagcodes), dtype=torch.long)\n",
        "    masks = torch.zeros((num_patients, max_num_visits, max_num_diagcodes), dtype=torch.bool)\n",
        "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_diagcodes), dtype=torch.long)\n",
        "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_diagcodes), dtype=torch.bool)\n",
        "    for i_patient, patient in enumerate(sequences):\n",
        "        for j_visit, visit in enumerate(patient):\n",
        "            for k_code, code in enumerate(visit):\n",
        "                 x[i_patient, j_visit, k_code] = code\n",
        "            masks[i_patient][j_visit][:len(visit)] = torch.Tensor([1]*len(visit))\n",
        "        rev_x[i_patient][:len(patient)] = torch.flip(x[i_patient][:len(patient)],[0])    \n",
        "        rev_masks[i_patient][:len(patient)] = torch.flip(masks[i_patient][:len(patient)],[0])    \n",
        "        \n",
        "    return x, masks, rev_x, rev_masks, y"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.CustomDataset object at 0x7f87e01fc250>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4DL3fOTRwsV"
      },
      "source": [
        "1.3 DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDXUkL0ERytM",
        "outputId": "b12a7084-6f5a-496b-85e2-9a1f2736e6b5"
      },
      "source": [
        "## DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "dataset = CustomDataset(seqs, morts)\n",
        "print(dataset)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)    \n",
        "#dataset_loader = iter(loader)\n",
        "#for i in range(0,10):\n",
        "#    x,masks,rev_x,rev_masks,y=next(dataset_loader)\n",
        "\n",
        "## load data\n",
        "from torch.utils.data.dataset import random_split\n",
        "split = int(len(dataset)*0.8)\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, val_dataset = random_split(dataset, lengths)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.CustomDataset object at 0x7f87e0187c90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0URvi_CR070"
      },
      "source": [
        "1.4 Split into train and val datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-rLCu_3uR29c",
        "outputId": "5e60c3e2-7326-4acc-dc15-bf7e5e01d23d"
      },
      "source": [
        "## split into train and val datasets\n",
        "from torch.utils.data import DataLoader\n",
        "def load_data(train_dataset, val_dataset, collate_fn):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
        "    #train_loader = iter(train_loader)\n",
        "    val_loader  = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "    #val_loader= iter(val_loader)\n",
        "    return(train_loader, val_loader)\n",
        "\n",
        "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n",
        "\n",
        "'''\n",
        "# testing to see how iterator works\n",
        "i = 0\n",
        "for step, batch,a,b,c in loader:\n",
        "    print('i=',i)\n",
        "    i = i + 1\n",
        "''' "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# testing to see how iterator works\\ni = 0\\nfor step, batch,a,b,c in loader:\\n    print('i=',i)\\n    i = i + 1\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geBgQkH6R980"
      },
      "source": [
        "2.1 RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVfv8QAjSApU"
      },
      "source": [
        "def sum_embeddings_with_mask(x, masks):\n",
        "    return torch.sum(x * masks.unsqueeze_(-1).expand(x.shape), dim=-2)\n",
        "\n",
        "def get_last_visit(hidden_states, masks):\n",
        "    sum_masks = masks.sum(axis=2)\n",
        "    last_true_visits = ((sum_masks > 0).sum(axis = 1) - 1)\n",
        "    last_true_visits = last_true_visits.view(-1, 1, 1).expand(hidden_states.shape)\n",
        "    out = torch.gather(hidden_states, dim=1, index=last_true_visits)\n",
        "    last_hidden_state = out[:, 0, :].squeeze()\n",
        "    return last_hidden_state\n",
        "\n",
        "class BaseRNN(nn.Module):\n",
        "    def __init__(self, num_codes):\n",
        "        super().__init__()\n",
        "        self.embDimSize = 32\n",
        "        self.embedding = nn.Embedding(num_embeddings = num_codes+1, embedding_dim = self.embDimSize)\n",
        "        self.rnn = nn.GRU(input_size = 32, hidden_size=32, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=32, out_features=1)\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "        embedding = self.embedding(x)\n",
        "        sum_embedding = sum_embeddings_with_mask(embedding, masks)\n",
        "        output, hidden = self.rnn(sum_embedding)\n",
        "        last_hidden = get_last_visit(output, masks)\n",
        "        fc = self.fc(last_hidden)\n",
        "        #return(self.sigmoid(fc).view(-1))\n",
        "        return(fc.view(-1))\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyAk5SZJSC4M"
      },
      "source": [
        "2.2 Loss, optimizer and model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3O1-g6jSG3N",
        "outputId": "fa911f8d-6a9a-46f6-f1fe-d0e864c86166"
      },
      "source": [
        "## load the model here\n",
        "model = BaseRNN(num_codes = num_types)\n",
        "print(model)\n",
        "\n",
        "## loss and optimizer\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
        "\n",
        "## eval_model\n",
        "from sklearn.metrics import *\n",
        "def regression_metrics(Y_pred, Y_True):\n",
        "    # Evaluation of methods: \n",
        "    # 1. Pearson's correlation (r), \n",
        "    # 2. Spearman's correlation (\u001a),\n",
        "    # 3. Mean absolute prediction error (MAPE),\n",
        "    # 4. R squared (r2),\n",
        "    # 5. Cumming's Prediction Measure (CPM)\n",
        "    mae, r2 = mean_absolute_error(Y_True, Y_pred), \\\n",
        "                r2_score(Y_True, Y_pred)\n",
        "    return mae, r2\n",
        "\n",
        "def eval_model(model, val_loader):\n",
        "    model.eval()\n",
        "    y_true = list()\n",
        "    y_pred = list()\n",
        "    #for x, y in val_loader:\n",
        "    for batch in val_loader:\n",
        "        xSUB, masksSUB, rev_xSUB, rev_masksSUB, labelsSUB = batch\n",
        "        with torch.no_grad():\n",
        "            pred = model(xSUB, masksSUB, rev_xSUB, rev_masksSUB)\n",
        "            y_true.extend(labelsSUB.detach().numpy().tolist())\n",
        "            y_pred.extend(pred.detach().numpy().reshape(-1).tolist())\n",
        "\n",
        "    mae, r2 = regression_metrics(y_pred, y_true)\n",
        "    return(mae, r2)\n",
        "print(eval_model(model, train_loader))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BaseRNN(\n",
            "  (embedding): Embedding(13367, 32)\n",
            "  (rnn): GRU(32, 32, batch_first=True)\n",
            "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "(1877.5939231460702, -0.013321903072579744)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7MHwptGSJTk"
      },
      "source": [
        "2.3 Model training, and test sample validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "wBXZR4L2SNtE",
        "outputId": "e759653d-5bae-4042-df14-ebd473a1d7c9"
      },
      "source": [
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss= 0\n",
        "        all_y_true = torch.LongTensor()\n",
        "        all_y_pred = torch.LongTensor()\n",
        "\n",
        "        for batch in train_loader:\n",
        "            xSUB, masksSUB, rev_xSUB, rev_masksSUB, y = batch\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(xSUB, masksSUB, rev_xSUB, rev_masksSUB)\n",
        "\n",
        "            y = y.view(y.shape[0])\n",
        "            #print(y_pred.shape)\n",
        "            #print(y.shape)\n",
        "            all_y_true = torch.cat((all_y_true, y.to('cpu').long()), dim=0)\n",
        "            all_y_pred = torch.cat((all_y_pred, y_pred.to('cpu').long()), dim=0)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_MAE, r2 = eval_model(model, train_loader)\n",
        "        val_MAE, r2 = eval_model(model, val_loader)\n",
        "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss} \\t Training MAE: {train_MAE} \\t Validation MAE: {val_MAE}')\n",
        "\n",
        "n_epochs = 25 \n",
        "train(model, train_loader, val_loader, n_epochs)\n",
        "\n",
        "test_mae, _ = eval_model(model, val_loader)\n",
        "print('Test MAE: %.2f'%(test_mae))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \t Training Loss: 1847.8011940331146 \t Training MAE: 1824.6236310742977 \t Validation MAE: 1752.9916416150552\n",
            "Epoch: 2 \t Training Loss: 1808.7991976921792 \t Training MAE: 1796.4273801128707 \t Validation MAE: 1724.423391162861\n",
            "Epoch: 3 \t Training Loss: 1788.5290145355018 \t Training MAE: 1779.4945938665269 \t Validation MAE: 1707.756992827019\n",
            "Epoch: 4 \t Training Loss: 1773.2840381025458 \t Training MAE: 1768.7862839207542 \t Validation MAE: 1698.5549432268674\n",
            "Epoch: 5 \t Training Loss: 1767.5847320902644 \t Training MAE: 1759.842841738822 \t Validation MAE: 1693.6497900031795\n",
            "Epoch: 6 \t Training Loss: 1756.102658987586 \t Training MAE: 1748.7064996376912 \t Validation MAE: 1689.0684605956742\n",
            "Epoch: 7 \t Training Loss: 1745.4524324032184 \t Training MAE: 1737.059284052921 \t Validation MAE: 1685.298776993227\n",
            "Epoch: 8 \t Training Loss: 1735.6582915636957 \t Training MAE: 1725.5150400325977 \t Validation MAE: 1682.1508143704302\n",
            "Epoch: 9 \t Training Loss: 1724.5841141958085 \t Training MAE: 1713.8124003182365 \t Validation MAE: 1680.3096788129872\n",
            "Epoch: 10 \t Training Loss: 1712.6678810379133 \t Training MAE: 1702.922481452489 \t Validation MAE: 1677.6891396628737\n",
            "Epoch: 11 \t Training Loss: 1702.6780833895243 \t Training MAE: 1693.3982820508288 \t Validation MAE: 1676.3385071339476\n",
            "Epoch: 12 \t Training Loss: 1693.4024887949972 \t Training MAE: 1682.1071910525754 \t Validation MAE: 1676.6471130468112\n",
            "Epoch: 13 \t Training Loss: 1684.9052001883924 \t Training MAE: 1673.7116640363424 \t Validation MAE: 1677.2528546674835\n",
            "Epoch: 14 \t Training Loss: 1674.614697272545 \t Training MAE: 1665.2892678926096 \t Validation MAE: 1675.643138894713\n",
            "Epoch: 15 \t Training Loss: 1665.416394707297 \t Training MAE: 1654.7669725760916 \t Validation MAE: 1677.1762243042247\n",
            "Epoch: 16 \t Training Loss: 1656.8197226318914 \t Training MAE: 1651.2832631459212 \t Validation MAE: 1676.3449026225362\n",
            "Epoch: 17 \t Training Loss: 1651.8401285807292 \t Training MAE: 1641.5695768547812 \t Validation MAE: 1677.8176344551475\n",
            "Epoch: 18 \t Training Loss: 1644.0317919119145 \t Training MAE: 1633.6971815131142 \t Validation MAE: 1680.8730201724086\n",
            "Epoch: 19 \t Training Loss: 1637.1197981023463 \t Training MAE: 1625.7472718743654 \t Validation MAE: 1679.7629325437385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fd06a9259080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtest_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-fd06a9259080>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, n_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}